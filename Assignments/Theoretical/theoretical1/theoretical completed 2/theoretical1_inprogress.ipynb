{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying tweets about Trump or Obama based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a tweet about Trump given that it contains the word \"president\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"Trump\"}|\\text{\"president\" in tweet}) = \\frac{P(\"\\text{\"president\" in tweet}|\\text{\"Trump\"})P(\\text{\"Trump\"})}{P(\"\\text{\"president\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a tweet about Trump given that it contains the word \"president\" \" we need the probability of \"president\" being in a tweet about Trump, the probability of a tweet being about Trump and the probability of \"president\" being in a tweet. \n",
    "\n",
    "Similarly if we want to obtain the opposite \"is this a tweet about Obama given that it contains the word \"president\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"Obama\"}|\\text{\"president\" in tweet}) = \\frac{P(\\text{\"president\" in tweet}|\\text{\"Obama\"})P(\\text{\"Obama\"})}{P(\\text{\"president\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"president\" being in a tweet about Obama, the probability of a tweet being about Obama and the probability of \"president\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"Trump\"|\"president\" in tweet) $>$ P(\"Obama\"|\"president\" in tweet)\n",
    "    \n",
    "   Tweet is about Trump\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is about Obama\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theroem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a tweet about Trump given that it contains the word \"president\" and \"America\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"Trump\"}|\\text{\"president\", \"America\" in tweet}) = \\frac{P(\\text{\"president\" in tweet}|\\text{\"Trump\"})P(\\text{\"America\" in tweet}|\\text{\"Trump\"})P(\\text{\"Trump\"})}{P(\\text{\"president\" in tweet})P(\\text{\"America\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:33:05.384466Z",
     "start_time": "2021-01-21T13:33:05.376779Z"
    }
   },
   "outputs": [],
   "source": [
    "#stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:33:05.634904Z",
     "start_time": "2021-01-21T13:33:05.628467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.4\n",
      "1.20.1\n",
      "0.24.1\n",
      "I assume we're good to go!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Looks like you don't have the same version of pandas as us!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-37204024be7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1.2.1\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"1.19.4\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"0.24.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I assume we're good to go!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1.2.1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Looks like you don't have the same version of pandas as us!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1.19.4\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Looks like you don't have the same version of numpy as us!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"0.24.0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Looks like you don't have the same version of sklearn as us!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Looks like you don't have the same version of pandas as us!"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)\n",
    "if (pd.__version__ > \"1.2.1\" and np.__version__ > \"1.19.4\" and sklearn.__version__ > \"0.24.0\"):\n",
    "    print(\"I assume we're good to go!\")\n",
    "assert pd.__version__ == \"1.2.1\", \"Looks like you don't have the same version of pandas as us!\"\n",
    "assert np.__version__ == \"1.19.4\", \"Looks like you don't have the same version of numpy as us!\"\n",
    "assert sklearn.__version__ == \"0.24.0\", \"Looks like you don't have the same version of sklearn as us!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:18.885001Z",
     "start_time": "2021-01-21T13:30:18.675615Z"
    }
   },
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('trump_20200530.csv')\n",
    "trump_tweets = df_t['text']\n",
    "df_t = pd.read_csv('Tweets-BarackObama.csv')\n",
    "obama_tweets = df_t['Tweet-text']\n",
    "\n",
    "tweet_data = trump_tweets.append(obama_tweets, ignore_index=True)\n",
    "tweet_labels = np.array(['T' for _ in range(len(trump_tweets))] + ['O' for _ in range(len(obama_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:20.197890Z",
     "start_time": "2021-01-21T13:30:20.181611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets about  O :  6851\n",
      "Number of tweets about  T :  18467\n"
     ]
    }
   ],
   "source": [
    "lab, counts = np.unique(tweet_labels, return_counts=True)\n",
    "print('Number of tweets about ', lab[0], ': ', counts[0])\n",
    "print('Number of tweets about ', lab[1], ': ', counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have many more Trump than Obama Tweets so simlpy guessing that a tweet is a Trump tweet already gives you a classifier that is correct about 70% of the time, but we can do better than this.\n",
    "\n",
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:22.469848Z",
     "start_time": "2021-01-21T13:30:22.450611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "18988\n",
      "6330\n"
     ]
    }
   ],
   "source": [
    "#Split data into train_tweets, test_tweets, train_labels and test_labels\n",
    "train_tweets, test_tweets, train_labels, test_labels = sklearn.model_selection.train_test_split(tweet_data, tweet_labels)\n",
    "#understanding the split function:\n",
    "test = sklearn.model_selection.train_test_split(tweet_labels)\n",
    "print(len(test))\n",
    "print(len(test[0]))\n",
    "print(len(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of tweet about Obama\" P(O) , \"probability of tweet about Trump\" P(T), \"probability of word in tweet given tweet about Obama\" P(w|O) and \"probability of word in tweet given tweet about Trump\" P(w|T). Start by calculating the probability that a tweet is about Obama and trump respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ob:6851, Tr:18467, total:25318\n",
      "0.2705979935223951 0.7294020064776049\n",
      "Number of tweets about  O :  5109\n",
      "Number of tweets about  T :  13879\n",
      "len T tweets: 13879 len O tweets: 5109\n",
      "43766\n",
      "912 1374 2286\n",
      "2286\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "Ob = Tr = total = 0\n",
    "if lab[0] == 'O' and lab[1] == 'T':\n",
    "    #We good!\n",
    "    Ob = counts[0]\n",
    "    Tr = counts[1]\n",
    "elif lab[1] == 'O' and lab[0] == 'T':\n",
    "    #we also good but they reversed... ok for now\n",
    "    Ob = counts[1]\n",
    "    Tr = counts[0]\n",
    "else:\n",
    "    raise Exception(\"Hey something is off with your data..\")\n",
    "total = Ob+Tr\n",
    "print(f\"Ob:{Ob}, Tr:{Tr}, total:{total}\")\n",
    "\n",
    "P_O = Ob/total\n",
    "P_T = Tr/total\n",
    "print(P_O, P_T)\n",
    "\n",
    "## No we can only use our test data!\n",
    "lab, counts = np.unique(train_labels, return_counts=True)\n",
    "print('Number of tweets about ', lab[0], ': ', counts[0])\n",
    "print('Number of tweets about ', lab[1], ': ', counts[1])\n",
    "total = len(train_tweets)\n",
    "Ob = counts[0]\n",
    "Tr = counts[1]\n",
    "P_O = Ob/total\n",
    "P_T = Tr/total\n",
    "\n",
    "train_O_tweets = []\n",
    "train_T_tweets = []\n",
    "index = 0\n",
    "for (tw,lb) in zip(train_tweets,train_labels):\n",
    "    if lb == 'T':\n",
    "        train_T_tweets.append(tw)\n",
    "    elif lb == 'O':\n",
    "        train_O_tweets.append(tw)\n",
    "    else:\n",
    "        raise Exception(\"Unexpected label!\")      \n",
    "    index+=1\n",
    "assert(index==total)\n",
    "print(f\"len T tweets: {len(train_T_tweets)} len O tweets: {len(train_O_tweets)}\")\n",
    "\n",
    "\n",
    "#create new dict with each word in it to count\n",
    "# got some help from https://www.geeksforgeeks.org/python-count-occurrences-of-each-word-in-given-text-file-using-dictionary/\n",
    "# as well as from https://datagy.io/python-remove-punctuation-from-string/\n",
    "# also this code is fairly repetitive, I should probably refactor it into some function... apologies\n",
    "word_occurrences = dict()\n",
    "for tw in train_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    for word in words:\n",
    "        if word in word_occurrences:\n",
    "            word_occurrences[word] = word_occurrences[word] + 1\n",
    "        else:\n",
    "            word_occurrences[word] = 1\n",
    "print(len(word_occurrences))\n",
    "#these are for all tweets. now let's create 2 more dict, one for t and one for O and count their word counts\n",
    "word_occurrences_T = dict()\n",
    "for tw in train_T_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    for word in words:\n",
    "        if word in word_occurrences_T:\n",
    "            word_occurrences_T[word] = word_occurrences_T[word] + 1\n",
    "        else:\n",
    "            word_occurrences_T[word] = 1\n",
    "word_occurrences_O = dict()\n",
    "for tw in train_O_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    for word in words:\n",
    "        if word in word_occurrences_O:\n",
    "            word_occurrences_O[word] = word_occurrences_O[word] + 1\n",
    "        else:\n",
    "            word_occurrences_O[word] = 1\n",
    "assert(word_occurrences_O['president'] + word_occurrences_T['president'] == word_occurrences['president'])\n",
    "print(word_occurrences_O['president'], word_occurrences_T['president'],word_occurrences_O['president']+word_occurrences_T['president'])\n",
    "print( word_occurrences['president']  )\n",
    "#print( word_occurrences )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_occurrences_unique = dict() #each word is only added maximum once per tweet here\n",
    "for tw in train_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    word_history = []\n",
    "    for word in words:\n",
    "        if word in word_occurrences_unique:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique[word] = word_occurrences_unique[word] + 1\n",
    "                word_history.append(word)\n",
    "        else:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique[word] = 1\n",
    "                word_history.append(word)\n",
    "    word_history = []\n",
    "\n",
    "\n",
    "\n",
    "word_occurrences_unique_O = dict() #each word is only added maximum once per tweet here\n",
    "for tw in train_O_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    word_history = []\n",
    "    for word in words:\n",
    "        if word in word_occurrences_unique_O:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique_O[word] = word_occurrences_unique_O[word] + 1\n",
    "                word_history.append(word)\n",
    "        else:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique_O[word] = 1\n",
    "                word_history.append(word)\n",
    "    word_history = []\n",
    "\n",
    "word_occurrences_unique_T = dict() #each word is only added maximum once per tweet here\n",
    "for tw in train_T_tweets:\n",
    "    tw = tw.lower()\n",
    "    tw = tw.translate(str.maketrans('', '', \"\"))\n",
    "    words = tw.split(\" \")\n",
    "    word_history = []\n",
    "    for word in words:\n",
    "        if word in word_occurrences_unique_T:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique_T[word] = word_occurrences_unique_T[word] + 1\n",
    "                word_history.append(word)\n",
    "        else:\n",
    "            if word not in word_history:\n",
    "                word_occurrences_unique_T[word] = 1\n",
    "                word_history.append(word)\n",
    "    word_history = []\n",
    "\n",
    "assert(word_occurrences_unique_O['president'] + word_occurrences_unique_T['president'] == word_occurrences_unique['president'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|O), P(w|T) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance  {'president': 'O':87, 'T': 100} meaning president occurs in 87 words about Obaman and 100 tweets about Trump. Be aware that we are not interested in calculating multiple occurances of the same word in the same tweet.  For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tweets obama: 6851 length of tweets trump:18467 \n",
      "attempt1{'trump': {'O': 5708, 'T': 6373}, 'word': {'O': 5708, 'T': 6373}, 'president': {'O': 5708, 'T': 6373}, 'hello': {'O': 5708, 'T': 6373}, 'obama': {'O': 5708, 'T': 6373}}\n",
      "attempt2{'president': ['O:2775', 'T:2180'], 'hello': ['O:5', 'T:9'], 'obama': ['O:2874', 'T:505'], 'trump': ['O:0', 'T:3534'], 'word': ['O:54', 'T:145']}\n",
      "attempt3{'president': {'O': 2775, 'T': 2180}, 'hello': {'O': 5, 'T': 9}, 'obama': {'O': 2874, 'T': 505}, 'trump': {'O': 0, 'T': 3534}, 'word': {'O': 54, 'T': 145}}\n",
      "attempt4{'O': 9, 'T': 38}\n",
      "attempt5{'O': 903, 'T': 1274}\n",
      "5109 13879\n"
     ]
    }
   ],
   "source": [
    "word_counter = {}\n",
    "print(f\"length of tweets obama: {len(obama_tweets)} length of tweets trump:{len(trump_tweets)} \")\n",
    "def word_counter_f(word, data):\n",
    "    count = 0\n",
    "    for string in data:\n",
    "        #s = (string.lower()).split(word.lower())\n",
    "        #if len(s) >= 2:\n",
    "            # if word exist, split and if len of split >= 2 we found the word in the tweet and can +1. \n",
    "            # dont care multiple occurances in same tweet otherwise we could count the splits. \n",
    "            # with that said, we don't really need split for this\n",
    "            #count +=1 \n",
    "        #alternatively\n",
    "        if word.lower() in string.lower():\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "#attempt 1\n",
    "word_list = [\"president\", \"hello\", \"obama\", \"trump\", \"word\"]\n",
    "word_counter = set()\n",
    "for w in word_list:\n",
    "    word_counter.add(w)\n",
    "word_counter = dict.fromkeys(word_counter, {'O':0,'T':0})\n",
    "for w in word_list:\n",
    "    word_counter[w]['O'] += word_counter_f(w, obama_tweets)\n",
    "    word_counter[w]['T'] += word_counter_f(w, trump_tweets)\n",
    "print(f\"attempt1{word_counter}\")\n",
    "\n",
    "#attempt 2\n",
    "word_counter = {}\n",
    "for w in word_list:\n",
    "    word_counter[w] = [f\"O:{(word_counter_f(w, obama_tweets))}\"]\n",
    "    word_counter[w].append( f\"T:{word_counter_f(w, trump_tweets)}\" )\n",
    "print(f\"attempt2{word_counter}\")\n",
    "\n",
    "#attempt 3\n",
    "word_counter = {}\n",
    "for w in word_list:\n",
    "    word_counter[w] = {'O':0,'T':0}\n",
    "for w in word_list:\n",
    "    word_counter[w]['O']+=(word_counter_f(w, obama_tweets))\n",
    "    word_counter[w]['T']+=(word_counter_f(w, trump_tweets))\n",
    "print(f\"attempt3{word_counter}\")\n",
    "\n",
    "\n",
    "#attempt 4\n",
    "word_counter = {}\n",
    "#obama\n",
    "for key in list(word_occurrences_O.keys()):\n",
    "    if key in word_counter:\n",
    "            word_counter[key]['O'] = word_occurrences_O[key] + 1\n",
    "    else:\n",
    "        word_counter[key] = {'O':0,'T':0}\n",
    "        word_counter[key]['O'] = word_counter[key]['O'] + word_occurrences_O[key]\n",
    "#trump\n",
    "for key in list(word_occurrences_T.keys()):\n",
    "    if key in word_counter:\n",
    "            word_counter[key]['T'] = word_occurrences_T[key] + 1\n",
    "    else:\n",
    "        word_counter[key] = {'O':0,'T':0}\n",
    "        word_counter[key]['T'] = word_counter[key]['O'] + word_occurrences_T[key]\n",
    "print(f\"attempt4{word_counter['word']}\")\n",
    "#print(f\"attempt4{word_counter}\")\n",
    "\n",
    "#attempt 5\n",
    "word_counter = {}\n",
    "#obama\n",
    "for key in list(word_occurrences_unique_O.keys()):\n",
    "    if key in word_counter:\n",
    "            word_counter[key]['O'] = word_occurrences_unique_O[key] + 1\n",
    "    else:\n",
    "        word_counter[key] = {'O':0,'T':0}\n",
    "        word_counter[key]['O'] = word_occurrences_unique_O[key]\n",
    "#trump\n",
    "for key in list(word_occurrences_unique_T.keys()):\n",
    "    if key in word_counter:\n",
    "            word_counter[key]['T'] = word_occurrences_unique_T[key] + 1\n",
    "    else:\n",
    "        word_counter[key] = {'O':0,'T':0}\n",
    "        word_counter[key]['T'] = word_occurrences_unique_T[key]\n",
    "print(f\"attempt5{word_counter['president']}\")\n",
    "print(len(train_O_tweets),len(train_T_tweets))\n",
    "\n",
    "\n",
    "#word_counter = {}\n",
    "train_data = train_tweets\n",
    "for (tweet, label) in zip(train_data, train_labels):\n",
    "    # ... Count number of tweets each word occurs in and store in word_counter where an entry looks like ex. {'word': 'O':98, 'T':10}\n",
    "    #i = 1\n",
    "    continue #do nothing\n",
    "    \n",
    "#print(word_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work with a smaller subset of words. Find the 100 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'of', 'a', 'in', 'is', 'rt', 'for', 'on', 'that', 'are', 'our', 'with', 'be', 'will', 'we', 'i', 'this', 'president', 'have', 'you', 'great', 'it', 'obama', 'at', 'they', 'has', '&amp;', 'was', 'all', 'not', 'by', 'from', 'people', 'just', '—president', 'he', 'who', 'as', 'very', 'your', 'my', 'their', 'more', 'about', 'no', 'thank', 'so', 'democrats', 'if', 'but', 'do', 'get', 'now', 'an', 'new', 'his', 'than', 'out', '-', 'trump', 'been', 'what', 'time', 'up', '', 'big', 'or', 'american', 'should', 'news', 'make', 'fake', 'many', 'can', 'one', 'would', 'today', 'country', 'want', 'never', 'there', 'house', 'when', '@realdonaldtrump', 'u.s.', 'america', 'congress', 'good', '@realdonaldtrump:', 'like', 'me', 'how', 'united', 'going', 'even', 'only', 'much', 'years']\n"
     ]
    }
   ],
   "source": [
    "nr_of_words_to_use = 100\n",
    "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['O'] + x[1]['T'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]\n",
    "print(popular_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute P(w|O), P(w|T) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_t = dict()\n",
    "P_w_given_o = dict()\n",
    "for word in popular_words:\n",
    "    #obama\n",
    "    if word in P_w_given_o:\n",
    "        assert(False) #should be first and only time\n",
    "    else:\n",
    "        word_occurrences = word_counter_f(word,train_O_tweets)\n",
    "        nr_of_tweets = len(train_O_tweets)\n",
    "        P_w_given_o[word] = word_occurrences/nr_of_tweets\n",
    "    #trump\n",
    "    if word in P_w_given_t:\n",
    "        assert(False)\n",
    "    else:\n",
    "        word_occurrences = word_counter_f(word,train_T_tweets)\n",
    "        nr_of_tweets = len(train_T_tweets)\n",
    "        P_w_given_t[word] = word_occurrences/nr_of_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis'  : popular_words,\n",
    "    'P(T)'   : P_O,\n",
    "    'P(O)'   : P_T,\n",
    "    'P(w|O)' : P_w_given_o,\n",
    "    'P(w|T)' : P_w_given_t\n",
    "    }   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Trump or Obama unsing the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )*  P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is about Obama or Trump i.e 'T' or 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(T)' - class probabilities\n",
    "                                          'P(O)' - class probabilities\n",
    "                                          'P(w|O)' - conditional probabilities\n",
    "                                          'P(w|T)' - conditional probabilities\n",
    "        \n",
    "        return: either 'T' or 'O'\n",
    "    \"\"\"\n",
    "    words_in_tweet = np.unique([x.lower() for x in tweet.split()])\n",
    "    \n",
    "    # ... Code for classifying tweets using the naive bayes classifier\n",
    "    \"\"\"if P(\"trump\"|\"president\" in tweet) > P(\"obama\"|\"president\" in tweet) \n",
    "    then tweet is about trump\"\"\"\n",
    "    nom_products = 1\n",
    "    for wd in words_in_tweet:\n",
    "        if wd in P_w_given_o:\n",
    "            nom_products = nom_products * P_w_given_o[wd] #P(w1|O) * ... * P(wn|O)\n",
    "    nom_products = nom_products * P_O #P_O\n",
    "    #TODO y = numpy.argmax() ....  not sure about this one for now\n",
    "    nominator = nom_products\n",
    "    denom_products = 1\n",
    "    for wd in words_in_tweet:\n",
    "        counts = word_counter_f(wd, train_data)\n",
    "        if counts > 0:\n",
    "            denom_products = denom_products * (counts/len(train_data))\n",
    "    denominator = denom_products\n",
    "    P_o_given_w = nominator/denominator\n",
    "\n",
    "    nom_products = 1\n",
    "    for wd in words_in_tweet:\n",
    "        if wd in P_w_given_t:\n",
    "            nom_products = nom_products * P_w_given_t[wd] #P(w1|T) * ... * P(wn|T)\n",
    "    nom_products = nom_products * P_T #P_T\n",
    "    #TODO y = numpy.argmax() ....  not sure about this one for now\n",
    "    nominator = nom_products\n",
    "    denom_products = 1\n",
    "    for wd in words_in_tweet:\n",
    "        counts = word_counter_f(wd, train_data)\n",
    "        if counts > 0:\n",
    "            denom_products = denom_products * (counts/len(train_data))\n",
    "        \n",
    "    denominator = denom_products\n",
    "    P_t_given_w = nominator/denominator\n",
    "\n",
    "    if P_t_given_w > P_o_given_w:\n",
    "        return 'T'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(T)' - class probabilities\n",
    "                                          'P(O)' - class probabilities\n",
    "                                          'P(w|O)' - conditional probabilities\n",
    "                                          'P(w|T)' - conditional probabilities\n",
    "        \n",
    "        return: either 'T' or 'O'\n",
    "    \"\"\"\n",
    "    words_in_tweet = np.unique([x.lower() for x in tweet.split()])\n",
    "    \n",
    "    # ... Code for classifying tweets using the naive bayes classifier\n",
    "    \"\"\"if P(\"trump\"|\"president\" in tweet) > P(\"obama\"|\"president\" in tweet) \n",
    "    then tweet is about trump\"\"\"   \n",
    "    list_of_probabilities = [] #P(\"president\" in tweet|\"trump\"), ... ,P(\"america\" in tweet|\"trump\")\n",
    "    for word in words_in_tweet:\n",
    "        try: \n",
    "            tweet_occurrences = word_occurrences_unique_O[word]\n",
    "            P_w_given_o = tweet_occurrences/len(word_occurrences_unique_O)\n",
    "            list_of_probabilities.append(P_w_given_o)\n",
    "        except KeyError:\n",
    "            #word does not exist in training data\n",
    "            list_of_probabilities.append(1) #we ignore new words by multiplying with 1 (new unseen words don't help with classification)\n",
    "    prob_products = math.prod(list_of_probabilities) #P(\"president\" in tweet|\"obama\")*...*P(\"america\" in tweet|\"obama\")\n",
    "    nominator_o = P_O * prob_products #P(\"obama) * P(\"president\" in tweet|\"obama\")*...*P(\"america\" in tweet|\"obama\")\n",
    "    \n",
    "    #trump\n",
    "    list_of_probabilities = [] #P(\"president\" in tweet|\"trump\"), ... ,P(\"america\" in tweet|\"trump\")\n",
    "    for word in words_in_tweet:\n",
    "        try: \n",
    "            tweet_occurrences = word_occurrences_unique_T[word]\n",
    "            P_w_given_t = tweet_occurrences/len(word_occurrences_unique_T)\n",
    "            list_of_probabilities.append(P_w_given_t)\n",
    "        except KeyError:\n",
    "            #word does not exist in training data\n",
    "            list_of_probabilities.append(1) #we ignore new words by multiplying with 1 (new unseen words don't help with classification)\n",
    "    prob_products = math.prod(list_of_probabilities) #P(\"president\" in tweet|\"obama\")*...*P(\"america\" in tweet|\"obama\")\n",
    "    nominator_t = P_T * prob_products #P(\"obama) * P(\"president\" in tweet|\"obama\")*...*P(\"america\" in tweet|\"obama\")\n",
    "\n",
    "\n",
    "\n",
    "    list_of_probabilities = [] #P(\"president\" in tweet), ... ,P(\"america\" in tweet)\n",
    "    for word in words_in_tweet:\n",
    "        try: \n",
    "            tweet_occurrences = word_occurrences_unique[word]\n",
    "            P_w = tweet_occurrences/len(word_occurrences_unique)\n",
    "            list_of_probabilities.append(P_w)\n",
    "        except KeyError:\n",
    "            #word does not exist in training data\n",
    "            list_of_probabilities.append(1) #we ignore new words by multiplying with 1 (new unseen words don't help with classification)\n",
    "    prob_products = math.prod(list_of_probabilities) #P(\"president\" in tweet|\"obama\")*...*P(\"america\" in tweet|\"obama\")\n",
    "    denominator = prob_products #P(\"president\" in tweet)*...*P(\"america\" in tweet)\n",
    "    #print(probs)\n",
    "    P_o_given_w = nominator_o/denominator\n",
    "    P_t_given_w = nominator_t/denominator\n",
    "\n",
    "\n",
    "    #print( )\n",
    "    #P_t_given_w = 0.7\n",
    "    #P_o_given_w = 0.3\n",
    "    if P_t_given_w > P_o_given_w:\n",
    "        return 'T'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet,label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet,classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-eaf787070bfb>:56: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  P_o_given_w = nominator_o/denominator\n",
      "<ipython-input-33-eaf787070bfb>:57: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  P_t_given_w = nominator_t/denominator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2295\n"
     ]
    }
   ],
   "source": [
    "acc = test_classifier(classifier, test_tweets, test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
